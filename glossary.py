GLOSSARY_TERMS = {

    "Accuracy": {
        "definition": (
            "Accuracy measures the proportion of correct predictions made by a model out of all predictions.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Overall Performance Metric</strong>: Calculates (Correct Predictions) / (Total Predictions)<br>"
            "‚Ä¢ <strong>Balanced Dataset Suitability</strong>: Most meaningful when classes are approximately equally distributed<br>"
            "‚Ä¢ <strong>Limitation with Imbalance</strong>: Can be misleading (e.g., 95% accuracy with 95% majority class)<br>"
            "‚Ä¢ <strong>Error Type Insensitivity</strong>: Doesn't differentiate between false positives and false negatives<br>"
            "‚Ä¢ <strong>Binary & Multiclass</strong>: Applicable to both binary and multiclass classification<br>"
            "‚Ä¢ <strong>Baseline Metric</strong>: Useful initial assessment but often insufficient alone<br><br>"
            "When to Use:<br>"
            "‚Ä¢ Balanced classification problems<br>"
            "‚Ä¢ Initial model assessment<br>"
            "‚Ä¢ Situations where all error types have equal cost<br><br>"
            "Mathematical Formulation:<br>"
            "Accuracy = (True Positives + True Negatives) / (TP + TN + False Positives + False Negatives)<br><br>"
            "Practical Example:<br>"
            "In a medical test with 100 patients (90 healthy, 10 diseased):<br>"
            "‚Ä¢ Model that predicts 'healthy' for everyone achieves 90% accuracy<br>"
            "‚Ä¢ This highlights why accuracy alone can be deceptive"
        ),
        "category": "Evaluation Metrics",
        "icon": "üéØ",
        "complexity": "Beginner",
        "related_terms": ["Precision", "Recall", "F1 Score", "Confusion Matrix"]
    },

    "Precision": {
        "definition": (
            "Precision (Positive Predictive Value) measures the proportion of true positive predictions among all positive predictions.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Positive Prediction Quality</strong>: Answers 'How reliable are our positive predictions?'<br>"
            "‚Ä¢ <strong>False Positive Focus</strong>: Penalizes false alarms (Type I errors)<br>"
            "‚Ä¢ <strong>Critical Applications</strong>: Spam detection, fraud prevention, content moderation<br>"
            "‚Ä¢ <strong>Trade-off with Recall</strong>: Increasing precision typically decreases recall<br>"
            "‚Ä¢ <strong>Class-Specific</strong>: Can be calculated per class in multiclass problems<br>"
            "‚Ä¢ <strong>Business Impact</strong>: High precision reduces operational costs from false alarms<br><br>"
            "When to Prioritize:<br>"
            "‚Ä¢ When false positives are expensive or harmful<br>"
            "‚Ä¢ Customer-facing automated decisions<br>"
            "‚Ä¢ Legal or compliance-sensitive applications<br><br>"
            "Mathematical Formulation:<br>"
            "Precision = True Positives / (True Positives + False Positives)<br><br>"
            "Example Scenario - Email Spam Filter:<br>"
            "‚Ä¢ High precision: When filter marks email as spam, it's almost certainly spam<br>"
            "‚Ä¢ Low precision: Many legitimate emails incorrectly marked as spam"
        ),
        "category": "Evaluation Metrics",
        "icon": "üî¨",
        "complexity": "Intermediate",
        "related_terms": ["Recall", "F1 Score", "Specificity", "Confusion Matrix"]
    },

    "Recall (Sensitivity)": {
        "definition": (
            "Recall measures the proportion of actual positives correctly identified by the model.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Completeness Metric</strong>: Answers 'What fraction of actual positives did we catch?'<br>"
            "‚Ä¢ <strong>False Negative Focus</strong>: Penalizes missed detections (Type II errors)<br>"
            "‚Ä¢ <strong>Critical Applications</strong>: Medical diagnosis, safety systems, search & retrieval<br>"
            "‚Ä¢ <strong>Alternative Names</strong>: Sensitivity, True Positive Rate, Hit Rate<br>"
            "‚Ä¢ <strong>Recall-Precision Trade-off</strong>: Capturing more positives usually increases false positives<br>"
            "‚Ä¢ <strong>Life-Saving Importance</strong>: Often prioritized in healthcare and safety-critical systems<br><br>"
            "When to Prioritize:<br>"
            "‚Ä¢ Medical screening (cancer detection)<br>"
            "‚Ä¢ Fraud detection where missed fraud is costly<br>"
            "‚Ä¢ Search engines (want all relevant documents)<br><br>"
            "Mathematical Formulation:<br>"
            "Recall = True Positives / (True Positives + False Negatives)<br><br>"
            "Example Scenario - Cancer Screening:<br>"
            "‚Ä¢ High recall: Very few cancer cases are missed (critical for patient safety)<br>"
            "‚Ä¢ Low recall: Many cancer cases go undetected (potentially fatal)"
        ),
        "category": "Evaluation Metrics",
        "icon": "üì°",
        "complexity": "Intermediate",
        "related_terms": ["Precision", "F1 Score", "Specificity", "ROC Curve"]
    },

    "F1 Score": {
        "definition": (
            "F1 Score is the harmonic mean of precision and recall, providing a single balanced metric.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Balanced Metric</strong>: Harmonically combines precision and recall<br>"
            "‚Ä¢ <strong>Imbalance Robustness</strong>: More informative than accuracy with skewed classes<br>"
            "‚Ä¢ <strong>Harmonic Mean Property</strong>: Penalizes extreme values more than arithmetic mean<br>"
            "‚Ä¢ <strong>Binary Focus</strong>: Originally for binary classification; multiclass requires averaging<br>"
            "‚Ä¢ <strong>Optimization Target</strong>: Often used when seeking balance between precision/recall<br>"
            "‚Ä¢ <strong>Limitation</strong>: Assumes equal importance of precision and recall<br><br>"
            "When to Use:<br>"
            "‚Ä¢ Class-imbalanced datasets<br>"
            "‚Ä¢ When both false positives and false negatives matter<br>"
            "‚Ä¢ Model comparison with single metric preference<br><br>"
            "Mathematical Formulation:<br>"
            "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)<br>"
            "FŒ≤ Score (generalized): (1+Œ≤¬≤) √ó (Precision √ó Recall) / (Œ≤¬≤√óPrecision + Recall)<br><br>"
            "Interpretation Guidelines:<br>"
            "‚Ä¢ F1 > 0.9: Excellent performance<br>"
            "‚Ä¢ F1 0.7-0.9: Good performance<br>"
            "‚Ä¢ F1 < 0.7: Needs improvement"
        ),
        "category": "Evaluation Metrics",
        "icon": "‚öñÔ∏è",
        "complexity": "Intermediate",
        "related_terms": ["Precision", "Recall", "FŒ≤ Score", "ROC-AUC"]
    },

    "AUC-ROC (Area Under ROC Curve)": {
        "definition": (
            "AUC-ROC measures a classifier's ability to distinguish between classes across all thresholds.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Threshold-Agnostic</strong>: Evaluates performance across all classification thresholds<br>"
            "‚Ä¢ <strong>Ranking Quality</strong>: Measures probability that positive samples rank higher than negatives<br>"
            "‚Ä¢ <strong>Scale Interpretation</strong>: 0.5 (random) to 1.0 (perfect discrimination)<br>"
            "‚Ä¢ <strong>Imbalance Resilience</strong>: Robust to class distribution changes<br>"
            "‚Ä¢ <strong>Visual Representation</strong>: ROC curve plots TPR vs FPR at various thresholds<br>"
            "‚Ä¢ <strong>Limitation</strong>: Can be optimistic with severe class imbalance<br><br>"
            "When to Use:<br>"
            "‚Ä¢ Binary classification model comparison<br>"
            "‚Ä¢ Threshold selection analysis<br>"
            "‚Ä¢ Medical test evaluation<br><br>"
            "Interpretation Guidelines:<br>"
            "‚Ä¢ AUC = 0.5: No discrimination (random)<br>"
            "‚Ä¢ AUC 0.7-0.8: Acceptable discrimination<br>"
            "‚Ä¢ AUC 0.8-0.9: Excellent discrimination<br>"
            "‚Ä¢ AUC > 0.9: Outstanding discrimination<br><br>"
            "ROC Curve Components:<br>"
            "‚Ä¢ X-axis: False Positive Rate (1 - Specificity)<br>"
            "‚Ä¢ Y-axis: True Positive Rate (Recall/Sensitivity)<br>"
            "‚Ä¢ Diagonal: Random classifier baseline"
        ),
        "category": "Evaluation Metrics",
        "icon": "üìà",
        "complexity": "Advanced",
        "related_terms": ["ROC Curve", "Precision-Recall Curve", "Specificity", "Youden's Index"]
    },

    "Overfitting": {
        "definition": (
            "Overfitting occurs when a model learns patterns specific to the training data that don't generalize.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>High Training Performance</strong>: Exceptionally low error on training data<br>"
            "‚Ä¢ <strong>Poor Generalization</strong>: High error on unseen validation/test data<br>"
            "‚Ä¢ <strong>Noise Learning</strong>: Model memorizes noise and outliers instead of signal<br>"
            "‚Ä¢ <strong>High Variance</strong>: Small data changes cause large model changes<br>"
            "‚Ä¢ <strong>Complexity Symptom</strong>: Often from overly complex models relative to data<br>"
            "‚Ä¢ <strong>Diagnostic Gap</strong>: Large gap between training and validation performance<br><br>"
            "Causes:<br>"
            "‚Ä¢ Insufficient training data<br>"
            "‚Ä¢ Excessive model complexity (too many parameters)<br>"
            "‚Ä¢ Training for too many epochs<br>"
            "‚Ä¢ Lack of regularization<br><br>"
            "Detection Methods:<br>"
            "‚Ä¢ Learning curves (train vs validation)<br>"
            "‚Ä¢ Cross-validation performance<br>"
            "‚Ä¢ Early stopping monitoring<br><br>"
            "Prevention Techniques:<br>"
            "‚Ä¢ Regularization (L1/L2, dropout)<br>"
            "‚Ä¢ Data augmentation<br>"
            "‚Ä¢ Feature selection<br>"
            "‚Ä¢ Ensemble methods<br>"
            "‚Ä¢ Early stopping"
        ),
        "category": "Model Behavior",
        "icon": "üìä",
        "complexity": "Intermediate",
        "related_terms": ["Underfitting", "Bias-Variance Tradeoff", "Regularization", "Early Stopping"]
    },

    "Underfitting": {
        "definition": (
            "Underfitting occurs when a model is too simple to capture underlying patterns in the data.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Poor Training Performance</strong>: High error even on training data<br>"
            "‚Ä¢ <strong>Poor Generalization</strong>: Also performs poorly on test data<br>"
            "‚Ä¢ <strong>High Bias</strong>: Strong assumptions prevent learning true relationships<br>"
            "‚Ä¢ <strong>Oversimplification</strong>: Model cannot represent necessary complexity<br>"
            "‚Ä¢ <strong>Systematic Error</strong>: Consistent prediction errors across datasets<br>"
            "‚Ä¢ <strong>Diagnostic Indicator</strong>: Both training and validation errors are high<br><br>"
            "Causes:<br>"
            "‚Ä¢ Excessively simple model architecture<br>"
            "‚Ä¢ Insufficient features or feature engineering<br>"
            "‚Ä¢ Excessive regularization<br>"
            "‚Ä¢ Training stopped too early<br><br>"
            "Detection Methods:<br>"
            "‚Ä¢ Learning curve analysis<br>"
            "‚Ä¢ Comparison with baseline models<br>"
            "‚Ä¢ Residual analysis<br><br>"
            "Remediation Strategies:<br>"
            "‚Ä¢ Increase model complexity<br>"
            "‚Ä¢ Add relevant features<br>"
            "‚Ä¢ Reduce regularization strength<br>"
            "‚Ä¢ Train for more epochs<br>"
            "‚Ä¢ Use ensemble methods"
        ),
        "category": "Model Behavior",
        "icon": "üìâ",
        "complexity": "Intermediate",
        "related_terms": ["Overfitting", "Bias-Variance Tradeoff", "Feature Engineering", "Model Complexity"]
    },

    "Bias (Statistical Bias)": {
        "definition": (
            "Bias represents error from erroneous assumptions in the learning algorithm.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Systematic Deviation</strong>: Consistent error in same direction<br>"
            "‚Ä¢ <strong>Underfitting Driver</strong>: High bias leads to underfitting<br>"
            "‚Ä¢ <strong>Assumption Error</strong>: Model assumptions don't match data reality<br>"
            "‚Ä¢ <strong>Irreducible with Data</strong>: More data alone doesn't fix high bias<br>"
            "‚Ä¢ <strong>Trade-off Component</strong>: Part of bias-variance trade-off<br>"
            "‚Ä¢ <strong>Examples</strong>: Linear model for nonlinear relationship<br><br>"
            "Types of Bias:<br>"
            "‚Ä¢ <strong>Algorithmic Bias</strong>: From model assumptions<br>"
            "‚Ä¢ <strong>Selection Bias</strong>: From non-random training data<br>"
            "‚Ä¢ <strong>Measurement Bias</strong>: From systematic measurement errors<br>"
            "‚Ä¢ <strong>Confirmation Bias</strong>: From reinforcing existing beliefs<br><br>"
            "Mathematical Representation:<br>"
            "Bias¬≤ = E[(fÃÇ(x) - f(x))¬≤] where f(x) is true function, fÃÇ(x) is estimate<br><br>"
            "Reduction Strategies:<br>"
            "‚Ä¢ More flexible models<br>"
            "‚Ä¢ Feature engineering<br>"
            "‚Ä¢ Ensemble methods (boosting)<br>"
            "‚Ä¢ Proper algorithm selection"
        ),
        "category": "Model Behavior",
        "icon": "‚ö°",
        "complexity": "Advanced",
        "related_terms": ["Variance", "Bias-Variance Tradeoff", "Underfitting", "Expected Error"]
    },

    "Variance": {
        "definition": (
            "Variance measures how much a model's predictions change with different training data.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Data Sensitivity</strong>: Model's sensitivity to specific training examples<br>"
            "‚Ä¢ <strong>Overfitting Driver</strong>: High variance leads to overfitting<br>"
            "‚Ä¢ <strong>Instability Indicator</strong>: Small data changes cause large prediction changes<br>"
            "‚Ä¢ <strong>Complexity Correlation</strong>: Increases with model complexity<br>"
            "‚Ä¢ <strong>Trade-off Component</strong>: Part of bias-variance trade-off<br>"
            "‚Ä¢ <strong>Reducible with Data</strong>: More data typically reduces variance<br><br>"
            "Mathematical Representation:<br>"
            "Variance = E[(fÃÇ(x) - E[fÃÇ(x)])¬≤] where fÃÇ(x) is model prediction<br><br>"
            "Sources of High Variance:<br>"
            "‚Ä¢ Too many parameters relative to data<br>"
            "‚Ä¢ Complex nonlinear models<br>"
            "‚Ä¢ Noisy training data<br>"
            "‚Ä¢ Insufficient regularization<br><br>"
            "Reduction Strategies:<br>"
            "‚Ä¢ More training data<br>"
            "‚Ä¢ Regularization techniques<br>"
            "‚Ä¢ Ensemble methods (bagging)<br>"
            "‚Ä¢ Feature selection<br>"
            "‚Ä¢ Simpler models<br><br>"
            "Expected Error Decomposition:<br>"
            "Expected Error = Bias¬≤ + Variance + Irreducible Error"
        ),
        "category": "Model Behavior",
        "icon": "üé≤",
        "complexity": "Advanced",
        "related_terms": ["Bias", "Bias-Variance Tradeoff", "Overfitting", "Regularization"]
    },

    "Regularization": {
        "definition": (
            "Regularization techniques prevent overfitting by adding constraints to model parameters.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Overfitting Prevention</strong>: Primary defense against high variance<br>"
            "‚Ä¢ <strong>Complexity Penalty</strong>: Adds penalty term to loss function<br>"
            "‚Ä¢ <strong>Parameter Constraint</strong>: Encourages smaller, simpler parameters<br>"
            "‚Ä¢ <strong>Generalization Improvement</strong>: Helps model generalize to unseen data<br>"
            "‚Ä¢ <strong>Trade-off Management</strong>: Balances fit and complexity<br>"
            "‚Ä¢ <strong>Universal Application</strong>: Used in linear models, neural networks, etc.<br><br>"
            "Common Regularization Techniques:<br>"
            "‚Ä¢ <strong>L1 Regularization (Lasso)</strong>: Adds absolute value penalty, promotes sparsity<br>"
            "‚Ä¢ <strong>L2 Regularization (Ridge)</strong>: Adds squared value penalty, smooths weights<br>"
            "‚Ä¢ <strong>Elastic Net</strong>: Combines L1 and L2 regularization<br>"
            "‚Ä¢ <strong>Dropout</strong>: Randomly ignores neurons during training (neural networks)<br>"
            "‚Ä¢ <strong>Early Stopping</strong>: Stops training when validation error increases<br>"
            "‚Ä¢ <strong>Data Augmentation</strong>: Artificially increases training data variety<br><br>"
            "Mathematical Form (L2):<br>"
            "Loss = Original Loss + Œª √ó Œ£(Œ∏·µ¢¬≤)<br>"
            "where Œª is regularization strength hyperparameter<br><br>"
            "Hyperparameter Tuning:<br>"
            "Regularization strength (Œª) is critical and requires careful cross-validation"
        ),
        "category": "Training Techniques",
        "icon": "üõ°Ô∏è",
        "complexity": "Intermediate",
        "related_terms": ["Overfitting", "L1 Regularization", "L2 Regularization", "Dropout", "Early Stopping"]
    },

    "Cross-Validation": {
        "definition": (
            "Cross-validation is a robust technique for assessing model performance on limited data.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Data Efficiency</strong>: Maximizes use of limited data for evaluation<br>"
            "‚Ä¢ <strong>Performance Estimation</strong>: Provides unbiased estimate of generalization error<br>"
            "‚Ä¢ <strong>Model Selection</strong>: Helps compare different models/algorithms<br>"
            "‚Ä¢ <strong>Hyperparameter Tuning</strong>: Essential for tuning model parameters<br>"
            "‚Ä¢ <strong>Variance Reduction</strong>: Multiple folds reduce evaluation variance<br>"
            "‚Ä¢ <strong>Computational Cost</strong>: Increases training time k-fold<br><br>"
            "Common Cross-Validation Methods:<br>"
            "‚Ä¢ <strong>k-Fold CV</strong>: Data split into k equal folds, each used as validation once<br>"
            "‚Ä¢ <strong>Stratified k-Fold</strong>: Preserves class distribution in each fold<br>"
            "‚Ä¢ <strong>Leave-One-Out (LOO)</strong>: Extreme case where k = n (computationally expensive)<br>"
            "‚Ä¢ <strong>Time Series CV</strong>: Special methods for temporal data<br>"
            "‚Ä¢ <strong>Nested CV</strong>: Outer loop for evaluation, inner loop for hyperparameter tuning<br><br>"
            "Best Practices:<br>"
            "‚Ä¢ Use stratified CV for imbalanced data<br>"
            "‚Ä¢ Shuffle data before splitting (except time series)<br>"
            "‚Ä¢ Report mean and standard deviation of scores<br>"
            "‚Ä¢ Ensure no data leakage between folds<br><br>"
            "Typical k Values:<br>"
            "‚Ä¢ Small datasets: 5 or 10 folds<br>"
            "‚Ä¢ Large datasets: 3 or 5 folds (computational constraints)<br>"
            "‚Ä¢ Very small datasets: Leave-One-Out"
        ),
        "category": "Evaluation Techniques",
        "icon": "üîÑ",
        "complexity": "Intermediate",
        "related_terms": ["Train-Test Split", "Hyperparameter Tuning", "Overfitting", "Model Selection"]
    },

    "Feature Engineering": {
        "definition": (
            "Feature engineering transforms raw data into informative features that improve model performance.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Performance Critical</strong>: Often more impactful than algorithm choice<br>"
            "‚Ä¢ <strong>Domain Knowledge Intensive</strong>: Requires understanding of problem context<br>"
            "‚Ä¢ <strong>Iterative Process</strong>: Continuous refinement based on model feedback<br>"
            "‚Ä¢ <strong>Creativity Required</strong>: Combines technical skill with insight<br>"
            "‚Ä¢ <strong>Automation Potential</strong>: Automated feature engineering tools emerging<br>"
            "‚Ä¢ <strong>Pipeline Essential</strong>: Core component of ML production pipelines<br><br>"
            "Common Feature Engineering Techniques:<br>"
            "‚Ä¢ <strong>Encoding</strong>: Categorical to numerical (one-hot, label, target encoding)<br>"
            "‚Ä¢ <strong>Scaling</strong>: Normalization, standardization for distance-based models<br>"
            "‚Ä¢ <strong>Interaction Features</strong>: Multiplying/dividing existing features<br>"
            "‚Ä¢ <strong>Polynomial Features</strong>: Creating squared/cubed terms for nonlinearity<br>"
            "‚Ä¢ <strong>Binning</strong>: Converting continuous to categorical<br>"
            "‚Ä¢ <strong>Date/Time Decomposition</strong>: Extracting day, month, season, etc.<br>"
            "‚Ä¢ <strong>Text Features</strong>: TF-IDF, n-grams, embeddings<br>"
            "‚Ä¢ <strong>Aggregation</strong>: Group statistics (mean, sum, count)<br><br>"
            "Best Practices:<br>"
            "‚Ä¢ Start with domain understanding<br>"
            "‚Ä¢ Create features that are interpretable<br>"
            "‚Ä¢ Avoid target leakage<br>"
            "‚Ä¢ Monitor feature importance<br>"
            "‚Ä¢ Regularize to prevent overfitting from many features"
        ),
        "category": "Data Preparation",
        "icon": "üîß",
        "complexity": "Intermediate",
        "related_terms": ["Feature Selection", "Normalization", "Encoding", "Dimensionality Reduction"]
    },

    "Hyperparameter": {
        "definition": (
            "Hyperparameters are configuration settings that control the learning process and must be set before training.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>External Configuration</strong>: Set by data scientist/engineer, not learned from data<br>"
            "‚Ä¢ <strong>Algorithm Control</strong>: Govern learning behavior and model capacity<br>"
            "‚Ä¢ <strong>Performance Critical</strong>: Significantly impact final model quality<br>"
            "‚Ä¢ <strong>Tuning Required</strong>: Optimal values found through systematic search<br>"
            "‚Ä¢ <strong>Problem-Specific</strong>: Optimal values vary by dataset and problem<br>"
            "‚Ä¢ <strong>Hierarchical</strong>: Some hyperparameters control others (e.g., network architecture)<br><br>"
            "Common Hyperparameters by Model Type:<br>"
            "‚Ä¢ <strong>Neural Networks</strong>: Learning rate, batch size, layers, neurons, dropout rate<br>"
            "‚Ä¢ <strong>Tree Models</strong>: Max depth, min samples split, number of estimators<br>"
            "‚Ä¢ <strong>SVMs</strong>: C (regularization), kernel type, gamma<br>"
            "‚Ä¢ <strong>k-NN</strong>: Number of neighbors, distance metric<br>"
            "‚Ä¢ <strong>Regularization Models</strong>: Lambda/alpha strength<br><br>"
            "Hyperparameter Tuning Methods:<br>"
            "‚Ä¢ <strong>Grid Search</strong>: Exhaustive search over predefined grid<br>"
            "‚Ä¢ <strong>Random Search</strong>: Random sampling of hyperparameter space<br>"
            "‚Ä¢ <strong>Bayesian Optimization</strong>: Probabilistic model-guided search<br>"
            "‚Ä¢ <strong>Genetic Algorithms</strong>: Evolutionary search approach<br>"
            "‚Ä¢ <strong>Hyperband</strong>: Adaptive resource allocation for tuning<br><br>"
            "Best Practices:<br>"
            "‚Ä¢ Use cross-validation for evaluation<br>"
            "‚Ä¢ Start with broad search, then refine<br>"
            "‚Ä¢ Consider computational constraints<br>"
            "‚Ä¢ Document tuning process and results"
        ),
        "category": "Model Configuration",
        "icon": "üéõÔ∏è",
        "complexity": "Intermediate",
        "related_terms": ["Learning Rate", "Cross-Validation", "Grid Search", "Bayesian Optimization"]
    },

    "Learning Rate": {
        "definition": (
            "Learning rate controls the step size during gradient-based optimization.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Optimization Control</strong>: Most critical hyperparameter for gradient descent<br>"
            "‚Ä¢ <strong>Step Size Determiner</strong>: How far to move in parameter space each update<br>"
            "‚Ä¢ <strong>Convergence Affector</strong>: Directly impacts training stability and speed<br>"
            "‚Ä¢ <strong>Problem-Sensitive</strong>: Optimal value varies by model and data<br>"
            "‚Ä¢ <strong>Adaptive Variants</strong>: Modern optimizers adapt learning rate during training<br>"
            "‚Ä¢ <strong>Schedule Potential</strong>: Can be decreased over time (learning rate decay)<br><br>"
            "Effects of Different Learning Rates:<br>"
            "‚Ä¢ <strong>Too High</strong>: Divergence, oscillations, failure to converge<br>"
            "‚Ä¢ <strong>Too Low</strong>: Slow convergence, risk of getting stuck in local minima<br>"
            "‚Ä¢ <strong>Optimal</strong>: Steady decrease in loss, efficient convergence<br><br>"
            "Common Learning Rate Schedules:<br>"
            "‚Ä¢ <strong>Constant</strong>: Fixed throughout training<br>"
            "‚Ä¢ <strong>Step Decay</strong>: Reduce by factor after fixed epochs<br>"
            "‚Ä¢ <strong>Exponential Decay</strong>: Continuous reduction<br>"
            "‚Ä¢ <strong>Cosine Annealing</strong>: Smooth periodic reduction<br>"
            "‚Ä¢ <strong>Cyclic</strong>: Oscillates between bounds (helps escape local minima)<br><br>"
            "Selection Guidelines:<br>"
            "‚Ä¢ Typical range: 0.1 to 0.0001<br>"
            "‚Ä¢ Start with 0.01 or 0.001 as baseline<br>"
            "‚Ä¢ Use learning rate finder techniques<br>"
            "‚Ä¢ Monitor loss curve for signs of instability<br><br>"
            "Mathematical Update Rule:<br>"
            "Œ∏ = Œ∏ - Œ∑ √ó ‚àáJ(Œ∏)<br>"
            "where Œ∑ is learning rate, ‚àáJ(Œ∏) is gradient"
        ),
        "category": "Optimization",
        "icon": "‚è±Ô∏è",
        "complexity": "Intermediate",
        "related_terms": ["Gradient Descent", "Optimizer", "Learning Rate Schedule", "Adam Optimizer"]
    },

    "Epoch": {
        "definition": (
            "An epoch represents one complete pass through the entire training dataset.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Training Unit</strong>: Fundamental measure of training progress<br>"
            "‚Ä¢ <strong>Full Dataset Usage</strong>: Model sees every training example once per epoch<br>"
            "‚Ä¢ <strong>Iterative Learning</strong>: Multiple epochs gradually improve performance<br>"
            "‚Ä¢ <strong>Monitoring Metric</strong>: Primary x-axis for training curves<br>"
            "‚Ä¢ <strong>Batch Relationship</strong>: One epoch contains multiple batches (batch updates)<br>"
            "‚Ä¢ <strong>Early Stopping Basis</strong>: Training stopped based on epoch-wise validation performance<br><br>"
            "Epoch vs Iteration vs Batch:<br>"
            "‚Ä¢ <strong>Batch</strong>: Subset of data used for one parameter update<br>"
            "‚Ä¢ <strong>Iteration</strong>: One parameter update (processing one batch)<br>"
            "‚Ä¢ <strong>Epoch</strong>: Number of iterations to process entire dataset<br><br>"
            "Calculating Iterations per Epoch:<br>"
            "Iterations per epoch = ceil(N / batch_size)<br>"
            "where N is total training samples<br><br>"
            "Epoch Strategy Considerations:<br>"
            "‚Ä¢ <strong>Too Few Epochs</strong>: Underfitting, model hasn't learned enough<br>"
            "‚Ä¢ <strong>Too Many Epochs</strong>: Overfitting, memorizes training data<br>"
            "‚Ä¢ <strong>Early Stopping</strong>: Stop when validation performance plateaus/worsens<br><br>"
            "Typical Epoch Ranges:<br>"
            "‚Ä¢ Simple models: 10-50 epochs<br>"
            "‚Ä¢ Deep learning: 50-500+ epochs<br>"
            "‚Ä¢ Large datasets: May use fewer epochs (data efficiency)<br><br>"
            "Monitoring During Training:<br>"
            "‚Ä¢ Track training loss per epoch<br>"
            "‚Ä¢ Monitor validation metrics<br>"
            "‚Ä¢ Watch for divergence or plateaus"
        ),
        "category": "Training Process",
        "icon": "üîÅ",
        "complexity": "Beginner",
        "related_terms": ["Batch Size", "Iteration", "Early Stopping", "Training Curve"]
    },

    "Batch Size": {
        "definition": (
            "Batch size determines how many training examples are processed before updating model parameters.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Memory Trade-off</strong>: Larger batches require more memory<br>"
            "‚Ä¢ <strong>Gradient Quality</strong>: Affects gradient estimate variance<br>"
            "‚Ä¢ <strong>Convergence Speed</strong>: Impacts number of updates per epoch<br>"
            "‚Ä¢ <strong>Hardware Utilization</strong>: Affects GPU/TPU efficiency<br>"
            "‚Ä¢ <strong>Generalization Impact</strong>: Influences model's final performance<br>"
            "‚Ä¢ <strong>Optimization Effect</strong>: Changes loss landscape traversal<br><br>"
            "Batch Size Spectrum:<br>"
            "‚Ä¢ <strong>Batch Gradient Descent</strong>: batch_size = N (entire dataset)<br>"
            "‚Ä¢ <strong>Mini-batch GD</strong>: 1 < batch_size < N (typical)<br>"
            "‚Ä¢ <strong>Stochastic GD</strong>: batch_size = 1<br><br>"
            "Effects on Training:<br>"
            "‚Ä¢ <strong>Small Batches</strong>:<br>"
            "  ‚Ä¢ Noisier gradients (regularization effect)<br>"
            "  ‚Ä¢ More updates per epoch<br>"
            "  ‚Ä¢ Better generalization often<br>"
            "  ‚Ä¢ Less memory required<br>"
            "‚Ä¢ <strong>Large Batches</strong>:<br>"
            "  ‚Ä¢ Smoother gradients<br>"
            "  ‚Ä¢ Faster computation (hardware optimization)<br>"
            "  ‚Ä¢ Potential generalization issues<br>"
            "  ‚Ä¢ More memory required<br><br>"
            "Selection Guidelines:<br>"
            "‚Ä¢ Start with 32 or 64 as baseline<br>"
            "‚Ä¢ Power of 2 for hardware optimization (32, 64, 128, 256)<br>"
            "‚Ä¢ Adjust based on memory constraints<br>"
            "‚Ä¢ Consider learning rate scaling: when increasing batch size, often increase learning rate<br><br>"
            "Batch Size Heuristics:<br>"
            "‚Ä¢ Small datasets: Smaller batches (16-64)<br>"
            "‚Ä¢ Large datasets: Larger batches (256-1024)<br>"
            "‚Ä¢ Deep learning: 32-256 common<br>"
            "‚Ä¢ Transfer learning: Often use original model's batch size"
        ),
        "category": "Training Process",
        "icon": "üì¶",
        "complexity": "Intermediate",
        "related_terms": ["Epoch", "Gradient Descent", "Learning Rate", "Memory Management"]
    },

    "Gradient Descent": {
        "definition": (
            "Gradient descent is an iterative optimization algorithm for minimizing differentiable functions.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>First-Order Optimization</strong>: Uses first derivatives (gradients)<br>"
            "‚Ä¢ <strong>Iterative Approach</strong>: Updates parameters repeatedly toward minimum<br>"
            "‚Ä¢ <strong>Local Optimization</strong>: Finds local minima (not necessarily global)<br>"
            "‚Ä¢ <strong>Foundation Algorithm</strong>: Basis for most neural network training<br>"
            "‚Ä¢ <strong>Gradient Requirement</strong>: Requires differentiable loss function<br>"
            "‚Ä¢ <strong>Backpropagation Partner</strong>: Gradient descent uses gradients computed via backpropagation<br><br>"
            "Core Update Rule:<br>"
            "Œ∏ = Œ∏ - Œ∑ √ó ‚àáJ(Œ∏)<br>"
            "where Œ∏ are parameters, Œ∑ is learning rate, ‚àáJ(Œ∏) is gradient of loss<br><br>"
            "Gradient Descent Variants:<br>"
            "‚Ä¢ <strong>Batch Gradient Descent</strong>: Uses entire dataset per update<br>"
            "  ‚Ä¢ Pros: Stable convergence, deterministic<br>"
            "  ‚Ä¢ Cons: Slow for large datasets, memory intensive<br>"
            "‚Ä¢ <strong>Stochastic GD (SGD)</strong>: Uses single sample per update<br>"
            "  ‚Ä¢ Pros: Fast updates, online learning possible<br>"
            "  ‚Ä¢ Cons: High variance, noisy convergence<br>"
            "‚Ä¢ <strong>Mini-batch GD</strong>: Uses small batch per update (most common)<br>"
            "  ‚Ä¢ Pros: Balance of speed and stability<br>"
            "  ‚Ä¢ Cons: Introduces batch size hyperparameter<br><br>"
            "Advanced Optimizers (GD Extensions):<br>"
            "‚Ä¢ <strong>Momentum</strong>: Accumulates velocity for faster convergence<br>"
            "‚Ä¢ <strong>Adam</strong>: Adaptive moments (most popular for deep learning)<br>"
            "‚Ä¢ <strong>RMSProp</strong>: Adaptive learning rate per parameter<br>"
            "‚Ä¢ <strong>Adagrad</strong>: Adapts learning rate based on parameter history<br><br>"
            "Challenges and Solutions:<br>"
            "‚Ä¢ <strong>Local Minima</strong>: Random initialization, momentum, restarts<br>"
            "‚Ä¢ <strong>Vanishing Gradients</strong>: ReLU activation, batch normalization<br>"
            "‚Ä¢ <strong>Learning Rate Selection</strong>: Learning rate schedules, adaptive methods"
        ),
        "category": "Optimization Algorithms",
        "icon": "‚¨áÔ∏è",
        "complexity": "Intermediate",
        "related_terms": ["Learning Rate", "Backpropagation", "Optimizer", "Loss Function"]
    },

    "Loss Function (Cost Function)": {
        "definition": (
            "A loss function quantifies how poorly a model's predictions match the true values.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Optimization Target</strong>: What the training algorithm minimizes<br>"
            "‚Ä¢ <strong>Differentiability Requirement</strong>: Must be differentiable for gradient-based methods<br>"
            "‚Ä¢ <strong>Problem-Specific</strong>: Choice depends on task type<br>"
            "‚Ä¢ <strong>Training Guide</strong>: Provides error signal for parameter updates<br>"
            "‚Ä¢ <strong>Evaluation Metric Relation</strong>: Often related to but different from evaluation metrics<br>"
            "‚Ä¢ <strong>Convexity Impact</strong>: Convex losses have single global minimum<br><br>"
            "Common Loss Functions by Task:<br>"
            "‚Ä¢ <strong>Regression Tasks</strong>:<br>"
            "  ‚Ä¢ Mean Squared Error (MSE): Emphasizes large errors<br>"
            "  ‚Ä¢ Mean Absolute Error (MAE): Robust to outliers<br>"
            "  ‚Ä¢ Huber Loss: Combines MSE and MAE benefits<br>"
            "‚Ä¢ <strong>Classification Tasks</strong>:<br>"
            "  ‚Ä¢ Binary Cross-Entropy: Binary classification standard<br>"
            "  ‚Ä¢ Categorical Cross-Entropy: Multiclass classification standard<br>"
            "  ‚Ä¢ Hinge Loss: Used in SVMs<br>"
            "‚Ä¢ <strong>Specialized Tasks</strong>:<br>"
            "  ‚Ä¢ Triplet Loss: Metric learning, face recognition<br>"
            "  ‚Ä¢ Contrastive Loss: Siamese networks<br>"
            "  ‚Ä¢ Focal Loss: Addresses class imbalance<br><br>"
            "Mathematical Examples:<br>"
            "‚Ä¢ MSE: (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤<br>"
            "‚Ä¢ Binary Cross-Entropy: -(1/n) Œ£[y·µ¢ log(≈∑·µ¢) + (1-y·µ¢) log(1-≈∑·µ¢)]<br><br>"
            "Loss Function Selection Criteria:<br>"
            "‚Ä¢ Task type (regression vs classification)<br>"
            "‚Ä¢ Output distribution assumptions<br>"
            "‚Ä¢ Robustness to outliers requirement<br>"
            "‚Ä¢ Computational considerations<br>"
            "‚Ä¢ Gradient behavior during training<br><br>"
            "Advanced Concepts:<br>"
            "‚Ä¢ Custom loss functions for specific constraints<br>"
            "‚Ä¢ Multi-task learning with combined losses<br>"
            "‚Ä¢ Regularization terms added to loss"
        ),
        "category": "Optimization",
        "icon": "üìê",
        "complexity": "Intermediate",
        "related_terms": ["Gradient Descent", "Optimizer", "Evaluation Metric", "Regularization"]
    },

    "Confusion Matrix": {
        "definition": (
            "A confusion matrix is a tabular visualization of classification model performance.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Error Analysis Tool</strong>: Reveals specific types of mistakes<br>"
            "‚Ä¢ <strong>Multiclass Extension</strong>: Can be extended beyond binary classification<br>"
            "‚Ä¢ <strong>Metric Foundation</strong>: Source for precision, recall, accuracy, F1<br>"
            "‚Ä¢ <strong>Visual Diagnostic</strong>: Easy to interpret error patterns<br>"
            "‚Ä¢ <strong>Imbalance Insight</strong>: Shows performance per class for imbalanced data<br>"
            "‚Ä¢ <strong>Threshold Analysis</strong>: Changes with classification threshold<br><br>"
            "Binary Confusion Matrix Structure:<br>"
            "‚Ä¢ <strong>True Positive (TP)</strong>: Correct positive predictions<br>"
            "‚Ä¢ <strong>True Negative (TN)</strong>: Correct negative predictions<br>"
            "‚Ä¢ <strong>False Positive (FP)</strong>: Negative incorrectly predicted as positive (Type I error)<br>"
            "‚Ä¢ <strong>False Negative (FN)</strong>: Positive incorrectly predicted as negative (Type II error)<br><br>"
            "Derived Metrics from Confusion Matrix:<br>"
            "‚Ä¢ Accuracy: (TP+TN) / Total<br>"
            "‚Ä¢ Precision: TP / (TP+FP)<br>"
            "‚Ä¢ Recall/Sensitivity: TP / (TP+FN)<br>"
            "‚Ä¢ Specificity: TN / (TN+FP)<br>"
            "‚Ä¢ F1 Score: Harmonic mean of precision and recall<br>"
            "‚Ä¢ False Positive Rate: FP / (FP+TN)<br><br>"
            "Multiclass Confusion Matrix:<br>"
            "‚Ä¢ Rows represent actual classes<br>"
            "‚Ä¢ Columns represent predicted classes<br>"
            "‚Ä¢ Diagonal shows correct predictions<br>"
            "‚Ä¢ Off-diagonal shows confusion between classes<br><br>"
            "Advanced Analysis Techniques:<br>"
            "‚Ä¢ Normalized confusion matrix (by row or column)<br>"
            "‚Ä¢ Per-class metrics calculation<br>"
            "‚Ä¢ Error pattern identification<br>"
            "‚Ä¢ Threshold optimization using matrix changes<br><br>"
            "Visualization Best Practices:<br>"
            "‚Ä¢ Use color scales for quick interpretation<br>"
            "‚Ä¢ Include numerical values in cells<br>"
            "‚Ä¢ Add marginal totals for context<br>"
            "‚Ä¢ Consider logarithmic scale for large value ranges"
        ),
        "category": "Evaluation Tools",
        "icon": "üßÆ",
        "complexity": "Beginner",
        "related_terms": ["Precision", "Recall", "Accuracy", "ROC Curve", "Classification Report"]
    },

    "Train-Test Split": {
        "definition": (
            "Train-test split divides data into separate sets for model training and evaluation.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Generalization Assessment</strong>: Evaluates performance on unseen data<br>"
            "‚Ä¢ <strong>Overfitting Detection</strong>: Reveals gap between training and test performance<br>"
            "‚Ä¢ <strong>Simple Implementation</strong>: Easy to implement and understand<br>"
            "‚Ä¢ <strong>Statistical Assumption</strong>: Assumes IID (independent and identically distributed) data<br>"
            "‚Ä¢ <strong>Variance Concern</strong>: Single split can give variable performance estimates<br>"
            "‚Ä¢ <strong>Data Leakage Prevention</strong>: Critical for avoiding overly optimistic estimates<br><br>"
            "Standard Split Ratios:<br>"
            "‚Ä¢ 80/20: Common default (80% train, 20% test)<br>"
            "‚Ä¢ 70/30: When more test data needed<br>"
            "‚Ä¢ 90/10: When data is limited<br>"
            "‚Ä¢ 60/20/20: With additional validation set<br><br>"
            "Split Methodologies:<br>"
            "‚Ä¢ <strong>Random Split</strong>: Most common, assumes IID data<br>"
            "‚Ä¢ <strong>Stratified Split</strong>: Preserves class distribution in both sets<br>"
            "‚Ä¢ <strong>Time-Based Split</strong>: For temporal data (train on past, test on future)<br>"
            "‚Ä¢ <strong>Grouped Split</strong>: Ensures same group doesn't appear in both sets<br>"
            "‚Ä¢ <strong>Geographic Split</strong>: For spatial data independence<br><br>"
            "Best Practices:<br>"
            "‚Ä¢ Perform split before any preprocessing<br>"
            "‚Ä¢ Use stratification for imbalanced classes<br>"
            "‚Ä¢ Set random seed for reproducibility<br>"
            "‚Ä¢ Consider dataset size when choosing ratio<br>"
            "‚Ä¢ Ensure no data leakage between sets<br><br>"
            "Limitations and Alternatives:<br>"
            "‚Ä¢ <strong>Single split variance</strong>: Use cross-validation for more stable estimates<br>"
            "‚Ä¢ <strong>Small datasets</strong>: Consider leave-one-out or bootstrap methods<br>"
            "‚Ä¢ <strong>Temporal data</strong>: Use time series cross-validation<br><br>"
            "Implementation Considerations:<br>"
            "‚Ä¢ sklearn.model_selection.train_test_split()<br>"
            "‚Ä¢ stratification parameter for balanced splits<br>"
            "‚Ä¢ shuffle parameter control<br>"
            "‚Ä¢ random_state for reproducibility"
        ),
        "category": "Evaluation Techniques",
        "icon": "‚úÇÔ∏è",
        "complexity": "Beginner",
        "related_terms": ["Cross-Validation", "Overfitting", "Data Leakage", "Stratified Sampling"]
    },

    "Normalization (Min-Max Scaling)": {
        "definition": (
            "Normalization rescales features to a fixed range, typically [0, 1].<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Range Transformation</strong>: Maps original range to [0,1] or other fixed interval<br>"
            "‚Ä¢ <strong>Distance Preservation</strong>: Maintains relative distances between values<br>"
            "‚Ä¢ <strong>Outlier Sensitivity</strong>: Highly sensitive to extreme values<br>"
            "‚Ä¢ <strong>Bounded Output</strong>: Result always within specified bounds<br>"
            "‚Ä¢ <strong>Interpretability</strong>: All features have same scale for comparison<br>"
            "‚Ä¢ <strong>Algorithm Suitability</strong>: Particularly useful for distance-based algorithms<br><br>"
            "Mathematical Formulation:<br>"
            "x' = (x - min(x)) / (max(x) - min(x))<br>"
            "Generalized to [a,b]: x' = a + (x - min(x)) √ó (b-a) / (max(x)-min(x))<br><br>"
            "When to Use Normalization:<br>"
            "‚Ä¢ Neural networks (helps gradient descent)<br>"
            "‚Ä¢ Distance-based algorithms (k-NN, k-means)<br>"
            "‚Ä¢ Algorithms requiring bounded input<br>"
            "‚Ä¢ When feature ranges vary significantly<br>"
            "‚Ä¢ Image pixel data (natural [0,255] range)<br><br>"
            "Comparison with Standardization:<br>"
            "‚Ä¢ <strong>Normalization</strong>: Bounded range, sensitive to outliers<br>"
            "‚Ä¢ <strong>Standardization</strong>: Unbounded, more robust to outliers<br>"
            "‚Ä¢ <strong>Choice depends</strong>: On algorithm and data characteristics<br><br>"
            "Practical Considerations:<br>"
            "‚Ä¢ Compute min/max on training set only<br>"
            "‚Ä¢ Apply same transformation to test data<br>"
            "‚Ä¢ Handle constant features (division by zero)<br>"
            "‚Ä¢ Consider robust min-max with percentiles for outlier handling<br><br>"
            "Implementation Example (scikit-learn):<br>"
            "from sklearn.preprocessing import MinMaxScaler<br>"
            "scaler = MinMaxScaler(feature_range=(0, 1))<br>"
            "X_train_scaled = scaler.fit_transform(X_train)<br>"
            "X_test_scaled = scaler.transform(X_test)<br><br>"
            "Alternative Normalization Methods:<br>"
            "‚Ä¢ Unit Vector normalization (L2 normalization)<br>"
            "‚Ä¢ Decimal scaling<br>"
            "‚Ä¢ Robust scaling (using percentiles)"
        ),
        "category": "Data Preprocessing",
        "icon": "üìè",
        "complexity": "Beginner",
        "related_terms": ["Standardization", "Feature Scaling", "Preprocessing", "Data Transformation"]
    },

    "Standardization (Z-score Normalization)": {
        "definition": (
            "Standardization transforms features to have zero mean and unit variance.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Gaussian Transformation</strong>: Assumes or creates approximately Gaussian distribution<br>"
            "‚Ä¢ <strong>Outlier Robustness</strong>: More robust to outliers than min-max scaling<br>"
            "‚Ä¢ <strong>Unbounded Output</strong>: Resulting values can exceed original range<br>"
            "‚Ä¢ <strong>Statistical Foundation</strong>: Based on mean and standard deviation<br>"
            "‚Ä¢ <strong>Algorithm Preference</strong>: Preferred by many linear models and SVMs<br>"
            "‚Ä¢ <strong>Interpretation</strong>: Values represent number of standard deviations from mean<br><br>"
            "Mathematical Formulation:<br>"
            "x' = (x - Œº) / œÉ<br>"
            "where Œº is mean, œÉ is standard deviation<br><br>"
            "When to Use Standardization:<br>"
            "‚Ä¢ Linear models (regression, logistic regression)<br>"
            "‚Ä¢ Support Vector Machines<br>"
            "‚Ä¢ Principal Component Analysis<br>"
            "‚Ä¢ When data contains outliers<br>"
            "‚Ä¢ When algorithm assumes standardized features<br><br>"
            "Statistical Properties:<br>"
            "‚Ä¢ Transformed features have mean = 0<br>"
            "‚Ä¢ Transformed features have variance = 1<br>"
            "‚Ä¢ Preserves shape of original distribution<br>"
            "‚Ä¢ Maintains relationships between features<br><br>"
            "Practical Implementation Considerations:<br>"
            "‚Ä¢ Fit scaler only on training data<br>"
            "‚Ä¢ Transform both training and test with same parameters<br>"
            "‚Ä¢ Handle near-constant features (small œÉ)<br>"
            "‚Ä¢ Consider robust standardization (median/IQR) for outliers<br><br>"
            "Comparison with Normalization:<br>"
            "‚Ä¢ <strong>Standardization</strong>: Better for outliers, unbounded, preserves distribution<br>"
            "‚Ä¢ <strong>Normalization</strong>: Bounded range, sensitive to outliers, changes distribution shape<br><br>"
            "Advanced Variants:<br>"
            "‚Ä¢ RobustScaler: Uses median and IQR instead of mean/std<br>"
            "‚Ä¢ QuantileTransformer: Maps to uniform/Gaussian distribution<br>"
            "‚Ä¢ PowerTransformer: Applies power transforms to normalize"
        ),
        "category": "Data Preprocessing",
        "icon": "üìä",
        "complexity": "Beginner",
        "related_terms": ["Normalization", "Gaussian Distribution", "Feature Scaling", "Preprocessing"]
    },

    "Ensemble Methods": {
        "definition": (
            "Ensemble methods combine multiple models to produce better predictions than any single model.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Wisdom of Crowds</strong>: Leverages multiple perspectives<br>"
            "‚Ä¢ <strong>Error Reduction</strong>: Reduces variance and/or bias<br>"
            "‚Ä¢ <strong>Robustness</strong>: More stable predictions<br>"
            "‚Ä¢ <strong>Performance</strong>: Often top performers in competitions<br>"
            "‚Ä¢ <strong>Complexity Cost</strong>: Increased computational requirements<br>"
            "‚Ä¢ <strong>Interpretability Challenge</strong>: Harder to explain than single models<br><br>"
            "Core Ensemble Strategies:<br>"
            "‚Ä¢ <strong>Bagging (Bootstrap Aggregating)</strong>:<br>"
            "  ‚Ä¢ Trains multiple models on different data subsets<br>"
            "  ‚Ä¢ Reduces variance<br>"
            "  ‚Ä¢ Examples: Random Forest, Extra Trees<br>"
            "‚Ä¢ <strong>Boosting</strong>:<br>"
            "  ‚Ä¢ Sequentially trains models focusing on previous errors<br>"
            "  ‚Ä¢ Reduces bias<br>"
            "  ‚Ä¢ Examples: AdaBoost, Gradient Boosting, XGBoost<br>"
            "‚Ä¢ <strong>Stacking</strong>:<br>"
            "  ‚Ä¢ Uses meta-model to combine base model predictions<br>"
            "  ‚Ä¢ Can combine different algorithm types<br>"
            "  ‚Ä¢ Powerful but complex<br>"
            "‚Ä¢ <strong>Voting/Averaging</strong>:<br>"
            "  ‚Ä¢ Simple combination of predictions<br>"
            "  ‚Ä¢ Hard voting (classification) or soft voting (probabilities)<br><br>"
            "Key Ensemble Algorithms:<br>"
            "‚Ä¢ <strong>Random Forest</strong>: Bagging of decision trees with feature randomness<br>"
            "‚Ä¢ <strong>Gradient Boosting Machines</strong>: Sequential tree building minimizing loss gradient<br>"
            "‚Ä¢ <strong>XGBoost</strong>: Optimized gradient boosting with regularization<br>"
            "‚Ä¢ <strong>LightGBM</strong>: Gradient boosting with leaf-wise tree growth<br>"
            "‚Ä¢ <strong>CatBoost</strong>: Gradient boosting optimized for categorical features<br><br>"
            "Ensemble Design Principles:<br>"
            "‚Ä¢ <strong>Diversity</strong>: Base models should make different errors<br>"
            "‚Ä¢ <strong>Competence</strong>: Each model should be reasonably accurate<br>"
            "‚Ä¢ <strong>Combination Strategy</strong>: How to aggregate predictions effectively<br><br>"
            "When to Use Ensembles:<br>"
            "‚Ä¢ When maximum accuracy is required<br>"
            "‚Ä¢ When computational resources allow<br>"
            "‚Ä¢ For competition settings<br>"
            "‚Ä¢ In production when stability is critical"
        ),
        "category": "Modeling Techniques",
        "icon": "ü§ù",
        "complexity": "Intermediate",
        "related_terms": ["Bagging", "Boosting", "Random Forest", "XGBoost", "Model Aggregation"]
    },

    "Clustering": {
        "definition": (
            "Clustering groups similar data points together without using predefined labels.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Unsupervised Learning</strong>: No target variable required<br>"
            "‚Ä¢ <strong>Exploratory Analysis</strong>: Discovers natural groupings in data<br>"
            "‚Ä¢ <strong>Similarity-Based</strong>: Groups based on distance or density<br>"
            "‚Ä¢ <strong>Dimensionality Tool</strong>: Can reveal structure in high-dimensional data<br>"
            "‚Ä¢ <strong>Preprocessing Step</strong>: Sometimes used before supervised learning<br>"
            "‚Ä¢ <strong>Validation Challenge</strong>: No ground truth for evaluation<br><br>"
            "Major Clustering Approaches:<br>"
            "‚Ä¢ <strong>Centroid-Based</strong>: Groups around central points<br>"
            "  ‚Ä¢ Examples: K-Means, K-Medoids<br>"
            "  ‚Ä¢ Pros: Simple, efficient<br>"
            "  ‚Ä¢ Cons: Assumes spherical clusters, sensitive to initialization<br>"
            "‚Ä¢ <strong>Density-Based</strong>: Groups based on density regions<br>"
            "  ‚Ä¢ Examples: DBSCAN, OPTICS<br>"
            "  ‚Ä¢ Pros: Handles arbitrary shapes, identifies outliers<br>"
            "  ‚Ä¢ Cons: Sensitive to parameters, struggles with varying densities<br>"
            "‚Ä¢ <strong>Hierarchical</strong>: Creates nested cluster tree<br>"
            "  ‚Ä¢ Examples: Agglomerative, Divisive<br>"
            "  ‚Ä¢ Pros: No need to specify k, dendrogram visualization<br>"
            "  ‚Ä¢ Cons: Computationally expensive, irreversible merges/splits<br>"
            "‚Ä¢ <strong>Distribution-Based</strong>: Assumes data from probability distributions<br>"
            "  ‚Ä¢ Examples: Gaussian Mixture Models<br>"
            "  ‚Ä¢ Pros: Soft clustering, probabilistic membership<br>"
            "  ‚Ä¢ Cons: Assumes distribution type<br><br>"
            "Clustering Evaluation Metrics:<br>"
            "‚Ä¢ <strong>Internal</strong>: Based on data itself (silhouette score, Davies-Bouldin)<br>"
            "‚Ä¢ <strong>External</strong>: When ground truth available (adjusted Rand index, NMI)<br>"
            "‚Ä¢ <strong>Relative</strong>: Compare different clusterings<br><br>"
            "Practical Considerations:<br>"
            "‚Ä¢ Feature scaling is usually necessary<br>"
            "‚Ä¢ Distance metric choice is critical<br>"
            "‚Ä¢ Determining optimal k is challenging (elbow method, silhouette analysis)<br>"
            "‚Ä¢ Interpret and validate clusters with domain knowledge<br><br>"
            "Common Applications:<br>"
            "‚Ä¢ Customer segmentation<br>"
            "‚Ä¢ Image segmentation<br>"
            "‚Ä¢ Document grouping<br>"
            "‚Ä¢ Anomaly detection<br>"
            "‚Ä¢ Social network analysis"
        ),
        "category": "Unsupervised Learning",
        "icon": "üé®",
        "complexity": "Intermediate",
        "related_terms": ["K-Means", "DBSCAN", "Hierarchical Clustering", "Dimensionality Reduction"]
    },

    "Dimensionality Reduction": {
        "definition": (
            "Dimensionality reduction transforms high-dimensional data into lower-dimensional representation.<br><br>"
            "Key Characteristics:<br>"
            "‚Ä¢ <strong>Curse Mitigation</strong>: Addresses curse of dimensionality<br>"
            "‚Ä¢ <strong>Visualization Enabler</strong>: Allows 2D/3D visualization of high-D data<br>"
            "‚Ä¢ <strong>Noise Reduction</strong>: Often removes noisy or redundant dimensions<br>"
            "‚Ä¢ <strong>Efficiency Improver</strong>: Speeds up training and inference<br>"
            "‚Ä¢ <strong>Structure Revealer</strong>: Can uncover hidden patterns<br>"
            "‚Ä¢ <strong>Information Trade-off</strong>: Balances compression with information preservation<br><br>"
            "Primary Dimensionality Reduction Techniques:<br>"
            "‚Ä¢ <strong>Linear Methods</strong>:<br>"
            "  ‚Ä¢ Principal Component Analysis (PCA): Orthogonal linear projection maximizing variance<br>"
            "  ‚Ä¢ Linear Discriminant Analysis (LDA): Supervised method maximizing class separation<br>"
            "  ‚Ä¢ Factor Analysis: Models observed variables with fewer latent factors<br>"
            "‚Ä¢ <strong>Nonlinear Methods</strong>:<br>"
            "  ‚Ä¢ t-SNE: Preserves local structure, excellent for visualization<br>"
            "  ‚Ä¢ UMAP: Preserves both local and global structure<br>"
            "  ‚Ä¢ Autoencoders: Neural network-based compression<br>"
            "  ‚Ä¢ Isomap: Preserves geodesic distances<br>"
            "  ‚Ä¢ LLE: Locally linear embedding<br><br>"
            "Selection Criteria:<br>"
            "‚Ä¢ <strong>PCA</strong>: When linear relationships dominate, for decorrelation<br>"
            "‚Ä¢ <strong>t-SNE</strong>: For visualization, exploring local structure<br>"
            "‚Ä¢ <strong>UMAP</strong>: For visualization with better global structure preservation<br>"
            "‚Ä¢ <strong>Autoencoders</strong>: When nonlinear relationships are complex<br>"
            "‚Ä¢ <strong>LDA</strong>: When class labels are available and separation is goal<br><br>"
            "Practical Considerations:<br>"
            "‚Ä¢ Scale features before linear methods<br>"
            "‚Ä¢ Determine optimal number of components (scree plot, cumulative variance)<br>"
            "‚Ä¢ t-SNE parameters (perplexity) significantly affect results<br>"
            "‚Ä¢ UMAP generally faster and more scalable than t-SNE<br>"
            "‚Ä¢ Reconstruction error for autoencoder evaluation<br><br>"
            "Applications:<br>"
            "‚Ä¢ Data visualization (exploratory analysis)<br>"
            "‚Ä¢ Feature extraction for downstream tasks<br>"
            "‚Ä¢ Data compression and storage<br>"
            "‚Ä¢ Noise filtering<br>"
            "‚Ä¢ Overcoming multicollinearity in regression<br><br>"
            "Limitations:<br>"
            "‚Ä¢ Information loss inevitable<br>"
            "‚Ä¢ Interpretability of reduced dimensions can be challenging<br>"
            "‚Ä¢ Some methods computationally intensive<br>"
            "‚Ä¢ Nonlinear methods may not preserve all relationships"
        ),
        "category": "Feature Engineering",
        "icon": "üóúÔ∏è",
        "complexity": "Advanced",
        "related_terms": ["PCA", "t-SNE", "UMAP", "Autoencoder", "Feature Extraction"]
    }
}